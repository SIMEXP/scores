#Scores tutorial
This tutorial is a working example of the scores pipeline and allows you to quickly run scores on a demo dataset and visualize the results. You can find the [matlab scrip version of the tutorial here](https://github.com/SIMEXP/Projects/blob/master/Scores/scores_tutorial.m). For a more theoretical introduction to scores, please refer to [this file](https://github.com/SIMEXP/Projects/wiki/Misc-demo-stable-cores).
## Notes
__You need niak installed to run this!__ Refer to [the support website](http://simexp.github.io/niak/niak_installation.html) for instructions on how to do this.
Additionally, since scores is still in development, you need to __checkout the scores branch__ of niak before you run. We will merge the scores branch very soon, so this should only be a problem for the immediate future.

 The tutorial includes code to collect preprocessed files from nested folders and generate the input structure expected by the pipeline. While this is not neccessary for the tutorial, it is intended as a template for your own experiments.

 Also, the tutorial is structured using matlabs cell structure. This means you can run different sections of the script at different times. If you run the whole script in one pass, the last visualization part will probably fail, because it is looking for the outputs of the pipeline that have not been completed yet. If you want to run the script in one go or use your own visualization tools, just comment out the last section.

 Lastly, please note that this script __will download data__ and generate output to __wherever you are executing it from__ - i.e. your current path in Matlab or Octave.

##Downloading the data
In this tutorial, we will use the cambridge resting state data. Scores works with any type of group template partition - in this case, we will use the [cambridge based basc partition](http://figshare.com/articles/Group_multiscale_functional_template_generated_with_BASC_on_the_Cambridge_sample/1285615) generated by our lab.

Run the following code in the tutorial:
```
if ~psom_exist('single_subject_cambridge_preprocessed_nii')
    system('wget http://www.nitrc.org/frs/download.php/6784/single_subject_cambridge_preprocessed_nii.zip')
    system('unzip single_subject_cambridge_preprocessed_nii.zip')
    psom_clean('single_subject_cambridge_preprocessed_nii.zip')
end
% Get the template
if ~psom_exist('template_cambridge_basc_multiscale_nii_sym')
    system('wget http://files.figshare.com/1861819/template_cambridge_basc_multiscale_nii_sym.zip')
    system('unzip template_cambridge_basc_multiscale_nii_sym.zip')
    psom_clean('template_cambridge_basc_multiscale_nii_sym.zip')
end
```
This will download both the cambridge group template as well as a single subject resting state file and unpack them wherever you are currently executing the script from. It won't download them again if the folder already exists. So if the downloaded file was somehow corrupted, please delete the unpacked folder and rerun the script.

##Setting up the inputs
The pipeline expects you to provide the path to all resting state input files in a structure (taken care of by the script), the path to the group template (that we just downloaded) and the desired output path. Run the following code to set this up:
```
in_path = [pwd filesep 'single_subject_cambridge_preprocessed_nii'];
part_path = [pwd filesep 'template_cambridge_basc_multiscale_nii_sym' filesep 'template_cambridge_basc_multiscale_sym_scale012.nii.gz'];
out_path = [pwd filesep 'output'];
```
As you can maybe see from the file name targeted in part_path, I chose to go with the cambridge template at scale 12. There are others available so you can try them out if you are interested.

This script allows you to search for inputs by regular expression matching. This can be useful if you have a large folder full of resting state files that you would like to process in bulk with the pipeline. You would need to adjust the search pattern in the next line
```
search_pattern = 'fmri_sub[0-9]*_session[0-9]+_rest.nii.gz';
```
to match your desired file names. This will be used by the Matlab regexp function, so refer to the help if you are unfamiliar with using the regex format.

Then we want to collect the files that match the search pattern and store them in a structure called in_files.fmri such that each input resting state file has it's own field in the structure (e.g. in_files.fmri.sub_1 = '../sub1_rest1.nii.gz'; in_files.fmri.sub_2 = '../sub2_rest1.nii.gz'). You can do this yourself or run the following code:
```
f = dir(in_path);
[~, path_name, ~] = niak_fileparts(in_path);
in_strings = {f.name};
in_strings = in_strings(3:end);
in_files.fmri = struct;
for f_id = 1:numel(in_strings)
    in_string = in_strings{f_id};
    tmp_path = [in_path filesep in_string];
    if isdir(tmp_path)
        % This is a subdirectory, we should see if there are other files inside
        [~, dir_name, ~] = niak_fileparts(tmp_path);
        f_dir = dir(tmp_path);
        dir_strings = {f_dir.name};
        for fd_id = 1:numel(dir_strings)
            dir_string = dir_strings{fd_id};
            [start, stop] = regexp(dir_string, search_pattern);
            if ~isempty(start) && ~isempty(stop)
                [~, fname, ~] = niak_fileparts(dir_string(start:stop));
                sub_name = [dir_name '_' fname];
        in_files.fmri.(sub_name) = [tmp_path filesep dir_string];
                a = [in_path filesep in_string];
            end
        end
    else
        [start, stop] = regexp(in_string, search_pattern);
        if ~isempty(start) && ~isempty(stop)
            [~, fname, ~] = niak_fileparts(in_string(start:stop));
            sub_name = [path_name '_' fname];
            in_files.fmri.(sub_name) = [in_path filesep in_string];
            a = [in_path filesep in_string];
        end
    end
end
```
There will probably be a warning about the names for the fields in the structure being too long. This is due to the length of the file name we are reading and doesn't affect the pipeline performance.

Lastly we will add the output path and the template partition to the input structure like so:
```
in_files.part = part_path;
opt.folder_out = out_path;
```

##Setting the options
The scores pipeline has a couple of options. You can [take a look at the source code (documentation is work in progress...)](https://github.com/SIMEXP/niak/blob/scores/extensions/surfstab/niak_pipeline_stability_scores.m) to get an overview of them. Most interesting will probably be the decision on whether or not to set the flag ```OPT.FLAG_TARGET```. If this flag is set to true (as is the case in our tutorial), then clusters are defined based on the similarity of the connectivity profile rather than the similarity of their time series. 

Because using ```OPT.FLAG_TARGET``` is requires you to provide a 4D partition, where the 2nd 3D element along the 4th dimension is the target with which the connectivity profiles are computed, the pipeline also has the flag ```OPT.FLAG_DEAL``` to deal with this for you. This is a bit of a lazy solution, since it just repeats a binarized version of your input partition such that everything inside you template partition will be considered the target region. This is however often what you want to do. If not, you can stack your template partition and target mask before running the pipeline and set ```OPT.FLAG_DEAL``` to false.

The other option specified in the script is not an option of the pipeline but of the pipeline manager, [psom](https://github.com/SIMEXP/psom). ```opt.psom.max_queued``` specifies the number of cores that will be used to execute jobs in the pipeline. This is set to ```1``` here, because we have only one file. If you run a lot of files, set this value to something higher - but keep into consideration that you have a limited number of cores on your machine that psom can use.

##Running the pipeline
Running the pipeline is now done by invoking the pipeline function and submitting the input structure ```in_files``` and the option structure ```opt``` like so:

```
pipeline = niak_pipeline_stability_scores(in_files, opt);
```

The pipeline will then ask you to confirm, that you want to run after checking all the inputs. Confirm by pressing ```Enter``` and then wait for the pipeline to complete.

If you are impatient like me, you can open a terminal and move to the output folder (current path + 'output') and find the 'logs' directory. Go in there and look for files with a .log ending. You can use ```tail -f some_job_name.log``` to keep a live feed of what the job does while it is running.
## Look at the outputs
The pipeline will dump a folder for each type of output in the output folder you specified. They are:

* dual_regression - outputs of dual regression
* rmap_part - seed maps based on each template network
* stability_contrast - silhouette maps (stability within cluster vs between)
* stability_inter - stability of a voxel with other clusters (should be low)
* stability_intra - stability of a voxel with it's own cluster (should be high)
* stability_maps - stability map of a given network
